{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f93d45-7e86-46b2-8f6c-8bc8534d1ee6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install PyGeodesy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f499d4-4cbf-4992-bcc5-a8de406fbd4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gpxpy\n",
    "import pandas as pd\n",
    "\n",
    "# Load GPX file\n",
    "file_path = \"YPTN_YPKU_41.gpx\"\n",
    "with open(file_path, 'r') as gpx_file:\n",
    "    gpx = gpxpy.parse(gpx_file)\n",
    "\n",
    "# Initialize lists to store GPS data\n",
    "data = {\n",
    "    'latitude': [],\n",
    "    'longitude': [],\n",
    "    'elevation': [],\n",
    "    'time': []\n",
    "}\n",
    "\n",
    "# Extract data from the GPX file\n",
    "for track in gpx.tracks:\n",
    "    for segment in track.segments:\n",
    "        for point in segment.points:\n",
    "            data['latitude'].append(point.latitude)\n",
    "            data['longitude'].append(point.longitude)\n",
    "            data['elevation'].append(point.elevation)\n",
    "            data['time'].append(point.time)\n",
    "\n",
    "# Convert the data into a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head()  # Show first few rows of the DataFrame\n",
    "\n",
    "\n",
    "import folium\n",
    "\n",
    "# Create a map centered around the first point\n",
    "m = folium.Map(location=[df['latitude'].iloc[0], df['longitude'].iloc[0]], zoom_start=13)\n",
    "\n",
    "# Add a polyline for the track\n",
    "folium.PolyLine(locations=list(zip(df['latitude'], df['longitude'])), color='blue').add_to(m)\n",
    "\n",
    "# Add markers for each point (optional)\n",
    "for idx, row in df.iterrows():\n",
    "    folium.Marker(location=[row['latitude'], row['longitude']], popup=str(row['time'])).add_to(m)\n",
    "\n",
    "# Display the map\n",
    "m.save(\"track_map.html\")  # Save the map to an HTML file\n",
    "m  # Display the map in a Jupyter Notebook (if applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9d0e3e-6945-4b66-a283-6f9b51c49c20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastkml import kml\n",
    "import folium\n",
    "\n",
    "# Load the KML file\n",
    "kml_file = 'Leg_7.kml'\n",
    "with open(kml_file, 'rt', encoding='utf-8') as f:\n",
    "    doc = f.read()\n",
    "\n",
    "# Parse the KML\n",
    "k = kml.KML()\n",
    "k.from_string(doc.encode('utf-8'))\n",
    "\n",
    "# Create a Folium map\n",
    "m = folium.Map(location=[-14.512871, 132.368211], zoom_start=15)\n",
    "\n",
    "# Extract coordinates from KML\n",
    "for feature in k.features():\n",
    "    for placemark in feature.features():\n",
    "        if hasattr(placemark, 'geometry'):\n",
    "            coords = placemark.geometry.coords\n",
    "            # Create a list of coordinates for the PolyLine\n",
    "            points = [(lat, lon) for lon, lat, alt in coords]  # Only take lat and lon\n",
    "            # Add the PolyLine to the map\n",
    "            folium.PolyLine(points, color='blue', weight=2.5, opacity=1).add_to(m)\n",
    "\n",
    "            # Optionally, print or store altitude data\n",
    "            # altitudes = [alt for lon, lat, alt in coords]\n",
    "            # print(\"Altitudes:\", altitudes)  # Print altitudes for reference\n",
    "\n",
    "# Save the map to an HTML file\n",
    "m.save('map_with_altitude.html')\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48939af-34de-4e02-b2b6-b8c88bcaa767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastkml import kml\n",
    "import folium\n",
    "import pandas as pd\n",
    "\n",
    "# Load the KML file\n",
    "kml_file = 'Leg_7.kml'\n",
    "with open(kml_file, 'rt', encoding='utf-8') as f:\n",
    "    doc = f.read()\n",
    "\n",
    "# Parse the KML\n",
    "k = kml.KML()\n",
    "k.from_string(doc.encode('utf-8'))\n",
    "\n",
    "# Initialize lists to store GPS data\n",
    "data = {\n",
    "    'latitude': [],\n",
    "    'longitude': [],\n",
    "    'elevation': [],\n",
    "    'time': []  # Assuming time exists, though it might not in some cases\n",
    "}\n",
    "\n",
    "# Recursive function to handle nested KML features and extract data\n",
    "def extract_features(kml_object):\n",
    "    for feature in kml_object.features():\n",
    "        if hasattr(feature, 'geometry') and feature.geometry is not None:\n",
    "            coords = feature.geometry.coords\n",
    "            for lon, lat, alt in coords:\n",
    "                data['latitude'].append(lat)\n",
    "                data['longitude'].append(lon)\n",
    "                data['elevation'].append(alt)\n",
    "                data['time'].append(None)  # Placeholder, you can modify if time data exists\n",
    "                \n",
    "        # If there are sub-features, recursively extract them\n",
    "        if hasattr(feature, 'features'):\n",
    "            extract_features(feature)\n",
    "\n",
    "# Start extraction from the top level of the KML object\n",
    "extract_features(k)\n",
    "\n",
    "# Convert the data dictionary to a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Create a Folium map centered on a default location\n",
    "m = folium.Map(location=[-14.512871, 132.368211], zoom_start=15)\n",
    "\n",
    "# Add the GPS track to the map\n",
    "for i, row in df.iterrows():\n",
    "    points = [(row['latitude'], row['longitude'])]\n",
    "    folium.PolyLine(points, color='blue', weight=2.5, opacity=1).add_to(m)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "m.save('map_with_altitude.html')\n",
    "\n",
    "# Display the map (if running in a Jupyter notebook)\n",
    "m  # Uncomment this line if running in Jupyter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7e2dbf-936f-405b-b006-13195cf6d123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpxpy\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "from math import atan2, degrees\n",
    "\n",
    "# Conversion factors\n",
    "METERS_TO_FEET = 3.28084\n",
    "KMH_TO_KNOTS = 0.539957\n",
    "\n",
    "# Load the GPX file and parse it using gpxpy\n",
    "gpx_file_path = 'YPTN_YPKU_41.gpx'\n",
    "with open(gpx_file_path, 'r') as gpx_file:\n",
    "    gpx = gpxpy.parse(gpx_file)\n",
    "\n",
    "# Initialize lists to store extracted data\n",
    "data = {\n",
    "    'latitude': [],\n",
    "    'longitude': [],\n",
    "    'elevation': [],\n",
    "    'time': [],\n",
    "    'geometry': []\n",
    "}\n",
    "\n",
    "# Extract the track points from the GPX file\n",
    "for track in gpx.tracks:\n",
    "    for segment in track.segments:\n",
    "        for point in segment.points:\n",
    "            data['latitude'].append(point.latitude)\n",
    "            data['longitude'].append(point.longitude)\n",
    "            data['elevation'].append(point.elevation)\n",
    "            data['time'].append(point.time)\n",
    "            data['geometry'].append((point.longitude, point.latitude))  # Store geometry as (lon, lat)\n",
    "\n",
    "# Convert to a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df, geometry=gpd.points_from_xy(df['longitude'], df['latitude'])\n",
    ")\n",
    "\n",
    "# Set the CRS to WGS84 (EPSG:4326)\n",
    "gdf.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "# Initialize lists for distance, time difference, speed, elevation change, angle, and converted units\n",
    "distances = []\n",
    "time_diffs = []\n",
    "speeds = []\n",
    "elevation_changes = []\n",
    "angles = []\n",
    "\n",
    "# Compute distance, time difference, speed, elevation change, and angle of elevation between consecutive points\n",
    "for i in range(1, len(gdf)):\n",
    "    # Get the current and previous points\n",
    "    point1 = (gdf.iloc[i-1]['latitude'], gdf.iloc[i-1]['longitude'])\n",
    "    point2 = (gdf.iloc[i]['latitude'], gdf.iloc[i]['longitude'])\n",
    "\n",
    "    # Calculate distance using geodesic method (in meters)\n",
    "    dist = geodesic(point1, point2).meters\n",
    "    distances.append(dist)\n",
    "\n",
    "    # Calculate time difference in seconds\n",
    "    time_diff = (gdf.iloc[i]['time'] - gdf.iloc[i-1]['time']).total_seconds()\n",
    "    time_diffs.append(time_diff)\n",
    "\n",
    "    # Calculate speed (m/s) as distance divided by time, and convert to km/h and knots\n",
    "    if time_diff > 0:\n",
    "        speed_kmh = (dist / time_diff) * 3.6  # Convert m/s to km/h\n",
    "        speed_knots = speed_kmh * KMH_TO_KNOTS  # Convert km/h to knots\n",
    "    else:\n",
    "        speed_knots = 0\n",
    "    speeds.append(speed_knots)\n",
    "\n",
    "    # Calculate elevation change (in meters)\n",
    "    elevation_change = gdf.iloc[i]['elevation'] - gdf.iloc[i-1]['elevation']\n",
    "    elevation_changes.append(elevation_change)\n",
    "\n",
    "    # Calculate angle of elevation (in degrees)\n",
    "    if dist > 0:  # Avoid division by zero\n",
    "        angle = degrees(atan2(elevation_change, dist))  # Use atan2 for angle\n",
    "    else:\n",
    "        angle = 0\n",
    "    angles.append(angle)\n",
    "\n",
    "# Add the calculated values to the GeoDataFrame\n",
    "gdf['distance_m'] = [0] + distances\n",
    "gdf['time_diff_sec'] = [0] + time_diffs\n",
    "gdf['speed_knots'] = [0] + speeds\n",
    "gdf['elevation_change_m'] = [0] + elevation_changes\n",
    "gdf['angle_deg'] = [0] + angles\n",
    "\n",
    "# Convert elevation from meters to feet\n",
    "gdf['elevation_ft'] = gdf['elevation'] * METERS_TO_FEET\n",
    "gdf['elevation_change_ft'] = gdf['elevation_change_m'] #iPad gpx file is in knots\n",
    "\n",
    "# Display the resulting GeoDataFrame\n",
    "print(gdf)\n",
    "\n",
    "# Optionally: Save the updated GeoDataFrame to a file or visualize using Folium\n",
    "import folium\n",
    "\n",
    "# Create a Folium map centered on the first point\n",
    "m = folium.Map(location=[gdf.iloc[0]['latitude'], gdf.iloc[0]['longitude']], zoom_start=10)\n",
    "\n",
    "# Add the GPS track to the map\n",
    "for i, row in gdf.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        popup=(\n",
    "            f\"Id: {i} \\n\"\n",
    "            f\"Elevation: {row['elevation_ft']} ft\\n\"\n",
    "            f\"Speed: {row['speed_knots']} knots\\n\"\n",
    "            f\"Elevation change: {row['elevation_change_ft']} ft\\n\"\n",
    "            f\"Angle: {row['angle_deg']}°\\n\"\n",
    "            f\"Time: {row['time']}\\n\"\n",
    "            f\"Time_int: {row['time_diff_sec']}\"\n",
    "        )\n",
    "    ).add_to(m)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "m.save('map_with_gpx_data.html')\n",
    "\n",
    "# Display the map (if running in a Jupyter notebook)\n",
    "m  # Uncomment if running in Jupyter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9007f0c9-378d-4826-aaca-a4cb4d688c54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gpxpy\n",
    "import gpxpy.gpx\n",
    "\n",
    "# Parsing an existing file:\n",
    "# -------------------------\n",
    "\n",
    "gpx_file = open('YPTN_YPKU_41.gpx', 'r')\n",
    "\n",
    "gpx = gpxpy.parse(gpx_file)\n",
    "\n",
    "for waypoint in gpx.waypoints:\n",
    "    print(f'waypoint {waypoint.name} -> ({waypoint.latitude},{waypoint.longitude})')\n",
    "    print(f'waypoint {waypoint.name} -> ({waypoint})')\n",
    "for route in gpx.routes:\n",
    "    print('Route:')\n",
    "    for point in route.points:\n",
    "        print(f'Point at ({point.latitude},{point.longitude}) -> {point.elevation}')\n",
    "        \n",
    "for track in gpx.tracks:\n",
    "    for segment in track.segments:\n",
    "        for point in segment.points:\n",
    "            print(f'Point at ({point.latitude},{point.longitude}) -> {point.elevation}')\n",
    "            for ext in point.extensions:\n",
    "                for extchild in list(ext):\n",
    "                    print('{0} -> {1}'.format(extchild.tag, extchild.text))\n",
    "\n",
    "\n",
    "print('GPX:', gpx.to_xml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fab6fd6-30d8-4b87-a33d-0496ddb8e9f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gpxpy\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "from math import atan2, degrees\n",
    "import xml.etree.ElementTree as ET\n",
    "import uuid\n",
    "\n",
    "# Conversion factors\n",
    "METERS_TO_FEET = 3.28084\n",
    "KMH_TO_KNOTS = 0.539956803455724\n",
    "MPS_TO_KMH = 3.6\n",
    "MPS_TO_KNOTS = 1.944\n",
    "\n",
    "# Gate data to load into the DataFrame\n",
    "gate_points = [\n",
    "    {'latitude': 34.0522, 'longitude': -118.2437, 'elevation': 100},\n",
    "    {'latitude': 40.7128, 'longitude': -74.0060, 'elevation': 100}\n",
    "]\n",
    "gate_time = 1000\n",
    "\n",
    "# Parse the GPX file\n",
    "# Load the GPX file and parse it using gpxpy\n",
    "gpx_file_path = 'YPTN_YPKU_41.gpx'\n",
    "with open(gpx_file_path, 'r') as gpx_file:\n",
    "    gpx = gpxpy.parse(gpx_file)\n",
    "    \n",
    "tree = ET.parse(gpx_file_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Define the namespace for GPX and AvPlan extensions\n",
    "namespace = {'gpx': 'http://www.topografix.com/GPX/1/1', 'avplan': 'avplan'}\n",
    "\n",
    "# Initialize a dictionary to store leg data\n",
    "leg_data = {\n",
    "    'leg_guid': [],\n",
    "    'leg_name': [],\n",
    "    'leg_number': [],\n",
    "    'src': [],\n",
    "    'desc': [],\n",
    "    'departure_time': [],\n",
    "    'holding_time': [],\n",
    "    'pob': [],\n",
    "    'taxi_time': [],\n",
    "    'block_off': [],\n",
    "    'wheels_on': [],\n",
    "    'wheels_off': [],\n",
    "    'taxi_fuel': [],\n",
    "    'approach_fuel': [],\n",
    "    'load_ids': [],\n",
    "    'load_values': [],\n",
    "    'fuel_load_ids': [],\n",
    "    'fuel_load_values': []\n",
    "}\n",
    "\n",
    "# Find the <rte> element\n",
    "for rte in root.findall('gpx:rte', namespace):\n",
    "    # Generate a GUID for each leg\n",
    "    leg_guid = str(uuid.uuid4())\n",
    "    \n",
    "    # Extract the leg name and number\n",
    "    leg_name = rte.find('gpx:name', namespace).text if rte.find('gpx:name', namespace) is not None else None\n",
    "    leg_number = rte.find('gpx:number', namespace).text if rte.find('gpx:number', namespace) is not None else None\n",
    "\n",
    "    # Extract the <AvPlanLegDetails> extension\n",
    "    leg_details = rte.find('gpx:extensions/avplan:AvPlanLegDetails', namespace)\n",
    "    if leg_details is not None:\n",
    "        src = leg_details.find('avplan:src', namespace).text if leg_details.find('avplan:src', namespace) is not None else None\n",
    "        desc = leg_details.find('avplan:desc', namespace).text if leg_details.find('avplan:desc', namespace) is not None else None\n",
    "        departure_time = leg_details.find('avplan:AvPlanDepartureTime', namespace).text if leg_details.find('avplan:AvPlanDepartureTime', namespace) is not None else None\n",
    "        holding_time = leg_details.find('avplan:AvPlanHoldingTime', namespace).text if leg_details.find('avplan:AvPlanHoldingTime', namespace) is not None else None\n",
    "        pob = leg_details.find('avplan:AvPlanPOB', namespace).text if leg_details.find('avplan:AvPlanPOB', namespace) is not None else None\n",
    "        taxi_time = leg_details.find('avplan:AvPlanTaxiTime', namespace).text if leg_details.find('avplan:AvPlanTaxiTime', namespace) is not None else None\n",
    "        block_off = leg_details.find('avplan:AvPlanBlockOff', namespace).text if leg_details.find('avplan:AvPlanBlockOff', namespace) is not None else None\n",
    "        wheels_on = leg_details.find('avplan:AvPlanWheelsOn', namespace).text if leg_details.find('avplan:AvPlanWheelsOn', namespace) is not None else None\n",
    "        wheels_off = leg_details.find('avplan:AvPlanWheelsOff', namespace).text if leg_details.find('avplan:AvPlanWheelsOff', namespace) is not None else None\n",
    "        taxi_fuel = leg_details.find('avplan:AvPlanTaxiFuel', namespace).text if leg_details.find('avplan:AvPlanTaxiFuel', namespace) is not None else None\n",
    "        approach_fuel = leg_details.find('avplan:AvPlanApproachFuel', namespace).text if leg_details.find('avplan:AvPlanApproachFuel', namespace) is not None else None\n",
    "\n",
    "        # Extract loading data\n",
    "        load_ids = []\n",
    "        load_values = []\n",
    "        for load in leg_details.findall('avplan:AvPlanLoading/avplan:AvPlanLoad', namespace):\n",
    "            load_id = load.find('avplan:AvPlanLoadID', namespace).text if load.find('avplan:AvPlanLoadID', namespace) is not None else None\n",
    "            load_value = load.find('avplan:AvPlanLoadValue', namespace).text if load.find('avplan:AvPlanLoadValue', namespace) is not None else None\n",
    "            load_ids.append(load_id)\n",
    "            load_values.append(load_value)\n",
    "\n",
    "        # Extract fuel loading data\n",
    "        fuel_load_ids = []\n",
    "        fuel_load_values = []\n",
    "        for fuel_load in leg_details.findall('avplan:AvPlanFuelLoading/avplan:AvPlanFuelLoad', namespace):\n",
    "            fuel_load_id = fuel_load.find('avplan:AvPlanFuelLoadID', namespace).text if fuel_load.find('avplan:AvPlanFuelLoadID', namespace) is not None else None\n",
    "            fuel_load_value = fuel_load.find('avplan:AvPlanFuelLoadValue', namespace).text if fuel_load.find('avplan:AvPlanFuelLoadValue', namespace) is not None else None\n",
    "            fuel_load_ids.append(fuel_load_id)\n",
    "            fuel_load_values.append(fuel_load_value)\n",
    "\n",
    "        # Append the extracted data to the dictionary\n",
    "        leg_data['leg_guid'].append(leg_guid)\n",
    "        leg_data['leg_name'].append(leg_name)\n",
    "        leg_data['leg_number'].append(leg_number)\n",
    "        leg_data['src'].append(src)\n",
    "        leg_data['desc'].append(desc)\n",
    "        leg_data['departure_time'].append(departure_time)\n",
    "        leg_data['holding_time'].append(holding_time)\n",
    "        leg_data['pob'].append(pob)\n",
    "        leg_data['taxi_time'].append(taxi_time)\n",
    "        leg_data['block_off'].append(block_off)\n",
    "        leg_data['wheels_on'].append(wheels_on)\n",
    "        leg_data['wheels_off'].append(wheels_off)\n",
    "        leg_data['taxi_fuel'].append(taxi_fuel)\n",
    "        leg_data['approach_fuel'].append(approach_fuel)\n",
    "        leg_data['load_ids'].append(load_ids)\n",
    "        leg_data['load_values'].append(load_values)\n",
    "        leg_data['fuel_load_ids'].append(fuel_load_ids)\n",
    "        leg_data['fuel_load_values'].append(fuel_load_values)\n",
    "\n",
    "# Convert the dictionary into a pandas DataFrame\n",
    "leg_df = pd.DataFrame(leg_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(leg_df)\n",
    "\n",
    "# Initialize a dictionary to store route point data\n",
    "route_data = {\n",
    "    'leg_guid': [],\n",
    "    'latitude': [],\n",
    "    'longitude': [],\n",
    "    'name': [],\n",
    "    'type': [],\n",
    "    'altitude': [],\n",
    "    'description': [],\n",
    "    'magvar': [],\n",
    "    'delay': [],\n",
    "    'alternate': [],\n",
    "    'lsalt': [],\n",
    "    'segment_rules': [],\n",
    "    'track': [],\n",
    "    'distance': [],\n",
    "    'distance_remaining': [],\n",
    "    'eta': [],\n",
    "    'ata': [],\n",
    "    'actual_fob': []\n",
    "}\n",
    "\n",
    "# Loop through all the <rtept> elements in the GPX file\n",
    "for rtept in root.findall('gpx:rte/gpx:rtept', namespace):\n",
    "    lat = rtept.get('lat')\n",
    "    lon = rtept.get('lon')\n",
    "    name = rtept.find('gpx:name', namespace).text if rtept.find('gpx:name', namespace) is not None else None\n",
    "    wpt_type = rtept.find('gpx:type', namespace).text if rtept.find('gpx:type', namespace) is not None else None\n",
    "\n",
    "    # Find the <AvPlanWaypointDetails> extension within <extensions>\n",
    "    waypoint_details = rtept.find('gpx:extensions/avplan:AvPlanWaypointDetails', namespace)\n",
    "    if waypoint_details is not None:\n",
    "        altitude = waypoint_details.find('avplan:AvPlanAltitude', namespace).text if waypoint_details.find('avplan:AvPlanAltitude', namespace) is not None else None\n",
    "        description = waypoint_details.find('avplan:desc', namespace).text if waypoint_details.find('avplan:desc', namespace) is not None else None\n",
    "        magvar = waypoint_details.find('avplan:magvar', namespace).text if waypoint_details.find('avplan:magvar', namespace) is not None else None\n",
    "        delay = waypoint_details.find('avplan:AvPlanDelay', namespace).text if waypoint_details.find('avplan:AvPlanDelay', namespace) is not None else None\n",
    "        alternate = waypoint_details.find('avplan:AvPlanAlternate', namespace).text if waypoint_details.find('avplan:AvPlanAlternate', namespace) is not None else None\n",
    "        lsalt = waypoint_details.find('avplan:AvPlanLSALT', namespace).text if waypoint_details.find('avplan:AvPlanLSALT', namespace) is not None else None\n",
    "        segment_rules = waypoint_details.find('avplan:AvPlanSegRules', namespace).text if waypoint_details.find('avplan:AvPlanSegRules', namespace) is not None else None\n",
    "        track = waypoint_details.find('avplan:AvPlanTrack', namespace).text if waypoint_details.find('avplan:AvPlanTrack', namespace) is not None else None\n",
    "        distance = waypoint_details.find('avplan:AvPlanDistance', namespace).text if waypoint_details.find('avplan:AvPlanDistance', namespace) is not None else None\n",
    "        distance_remaining = waypoint_details.find('avplan:AvPlanDistanceRemain', namespace).text if waypoint_details.find('avplan:AvPlanDistanceRemain', namespace) is not None else None\n",
    "        eta = waypoint_details.find('avplan:AvPlanETA', namespace).text if waypoint_details.find('avplan:AvPlanETA', namespace) is not None else None\n",
    "        ata = waypoint_details.find('avplan:AvPlanATA', namespace).text if waypoint_details.find('avplan:AvPlanATA', namespace) is not None else None\n",
    "        actual_fob = waypoint_details.find('avplan:AvPlanActualFOB', namespace).text if waypoint_details.find('avplan:AvPlanActualFOB', namespace) is not None else None\n",
    "\n",
    "        # Append the extracted data to the dictionary\n",
    "        route_data['leg_guid'].append(leg_guid)\n",
    "        route_data['latitude'].append(float(lat))\n",
    "        route_data['longitude'].append(float(lon))\n",
    "        route_data['name'].append(name)\n",
    "        route_data['type'].append(wpt_type)\n",
    "        route_data['altitude'].append(float(altitude) if altitude else None)\n",
    "        route_data['description'].append(description)\n",
    "        route_data['magvar'].append(float(magvar) if magvar else None)\n",
    "        route_data['delay'].append(float(delay) if delay else None)\n",
    "        route_data['alternate'].append(float(alternate) if alternate else None)\n",
    "        route_data['lsalt'].append(float(lsalt) if lsalt else None)\n",
    "        route_data['segment_rules'].append(segment_rules)\n",
    "        route_data['track'].append(float(track) if track else None)\n",
    "        route_data['distance'].append(float(distance) if distance else None)\n",
    "        route_data['distance_remaining'].append(float(distance_remaining) if distance_remaining else None)\n",
    "        route_data['eta'].append(eta)\n",
    "        route_data['ata'].append(ata)\n",
    "        route_data['actual_fob'].append(float(actual_fob) if actual_fob else None)\n",
    "\n",
    "# Convert the dictionary into a pandas DataFrame\n",
    "route_df = pd.DataFrame(route_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(route_df)\n",
    "\n",
    "\n",
    "# Initialize lists to store extracted data\n",
    "data = {\n",
    "    'leg_guid': [],\n",
    "    'latitude': [],\n",
    "    'longitude': [],\n",
    "    'elevation_ft': [],\n",
    "    'time': [],\n",
    "    'speed_avplan': [],\n",
    "    'heading': [],\n",
    "    'hdop': [],\n",
    "    'vdop': [],\n",
    "    'geometry': []\n",
    "}\n",
    "\n",
    "# Extract the track points from the GPX file\n",
    "for track in gpx.tracks:\n",
    "    for segment in track.segments:\n",
    "        for point in segment.points:\n",
    "            data['leg_guid'].append(leg_guid)\n",
    "            data['latitude'].append(point.latitude)\n",
    "            data['longitude'].append(point.longitude)\n",
    "            data['elevation_ft'].append(point.elevation)  # Assume elevation is already in feet\n",
    "            data['time'].append(point.time)\n",
    "\n",
    "            # Extract HDOP, VDOP, and heading from the extensions if available\n",
    "            hdop = None\n",
    "            vdop = None\n",
    "            heading = None\n",
    "            speed_knots = None\n",
    "\n",
    "            if point.extensions:\n",
    "                for ext in point.extensions:\n",
    "                    tree = ET.ElementTree(ET.fromstring(ET.tostring(ext)))\n",
    "                    root = tree.getroot()\n",
    "\n",
    "                    # Extract speed (knots), heading, hdop, and vdop from extensions\n",
    "                    speed_elem = root.find('.//{avplan}speed')\n",
    "                    heading_elem = root.find('.//{avplan}heading')\n",
    "                    hdop_elem = root.find('.//hdop')\n",
    "                    vdop_elem = root.find('.//vdop')\n",
    "\n",
    "                    if speed_elem is not None:\n",
    "                        speed_avplan = float(speed_elem.text)\n",
    "                    if heading_elem is not None:\n",
    "                        heading = float(heading_elem.text)\n",
    "                    if hdop_elem is not None:\n",
    "                        hdop = float(hdop_elem.text)\n",
    "                    if vdop_elem is not None:\n",
    "                        vdop = float(vdop_elem.text)\n",
    "\n",
    "            data['speed_avplan'].append(speed_avplan)\n",
    "            data['heading'].append(heading)\n",
    "            data['hdop'].append(hdop)\n",
    "            data['vdop'].append(vdop)\n",
    "            data['geometry'].append((point.longitude, point.latitude))  # Store geometry as (lon, lat)\n",
    "\n",
    "# Convert to a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df, geometry=gpd.points_from_xy(df['longitude'], df['latitude'])\n",
    ")\n",
    "\n",
    "# Set the CRS to WGS84 (EPSG:4326)\n",
    "gdf.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "# Initialize lists for distance, time difference, elevation change, and angle of elevation\n",
    "distances = []\n",
    "speed_calc = []\n",
    "time_diffs = []\n",
    "elevation_changes = []\n",
    "angles = []\n",
    "\n",
    "# Compute distance, time difference, elevation change, and angle of elevation between consecutive points\n",
    "for i in range(1, len(gdf)):\n",
    "    # Get the current and previous points\n",
    "    point1 = (gdf.iloc[i-1]['latitude'], gdf.iloc[i-1]['longitude'])\n",
    "    point2 = (gdf.iloc[i]['latitude'], gdf.iloc[i]['longitude'])\n",
    "\n",
    "    # Calculate distance using geodesic method (in meters)\n",
    "    dist = geodesic(point1, point2).meters\n",
    "    distances.append(dist)\n",
    "\n",
    "    # Calculate time difference in seconds\n",
    "    time_diff = (gdf.iloc[i]['time'] - gdf.iloc[i-1]['time']).total_seconds()\n",
    "    time_diffs.append(time_diff)\n",
    "    \n",
    "    # Calculate speed (m/s) as distance divided by time, and convert to km/h and knots\n",
    "    if time_diff > 0:\n",
    "        speed_mps = (dist / time_diff)\n",
    "        speed_kmh =  speed_mps * MPS_TO_KMH  # Convert m/s to km/h\n",
    "        speed_knots = speed_kmh * KMH_TO_KNOTS  # Convert km/h to knots\n",
    "    else:\n",
    "        speed_knots = 0\n",
    "    speed_calc.append(speed_mps)\n",
    "\n",
    "    # Calculate elevation change (in feet)\n",
    "    elevation_change = gdf.iloc[i]['elevation_ft'] - gdf.iloc[i-1]['elevation_ft']\n",
    "    elevation_changes.append(elevation_change)\n",
    "\n",
    "    # Calculate angle of elevation (in degrees)\n",
    "    if dist > 0:  # Avoid division by zero\n",
    "        angle = degrees(atan2(elevation_change, dist))  # Use atan2 for angle\n",
    "    else:\n",
    "        angle = 0\n",
    "    angles.append(angle)\n",
    "\n",
    "# Add the calculated values to the GeoDataFrame\n",
    "gdf['distance_m'] = [0] + distances\n",
    "gdf['speed_calc'] = [0] + speed_calc\n",
    "gdf['time_diff_sec'] = [0] + time_diffs\n",
    "gdf['elevation_change_ft'] = [0] + elevation_changes\n",
    "gdf['angle_deg'] = [0] + angles\n",
    "\n",
    "# Display the resulting GeoDataFrame\n",
    "print(gdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1d6eda5-9742-4d38-931c-9a6bb4e39656",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'route_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 116\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Extract the gate points from the route data where name matches the pattern\u001b[39;00m\n\u001b[1;32m    115\u001b[0m gate_points \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m route_df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_in_or_out_with_number(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;66;03m# Create a dictionary with the latitude, longitude, and elevation\u001b[39;00m\n\u001b[1;32m    119\u001b[0m         gate_point \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    120\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    121\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    122\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124melevation\u001b[39m\u001b[38;5;124m'\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maltitude\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Assuming 'altitude' is the correct field for elevation\u001b[39;00m\n\u001b[1;32m    123\u001b[0m         }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'route_df' is not defined"
     ]
    }
   ],
   "source": [
    "import gpxpy\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from pygeodesy.ellipsoidalVincenty import LatLon\n",
    "from pygeodesy import sphericalTrigonometry as st\n",
    "from geopy.distance import geodesic\n",
    "from math import atan2, degrees\n",
    "import folium\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "\n",
    "# Function to find the two closest points on the track\n",
    "def find_closest_points(gate_point, gdf):\n",
    "    gate_lat, gate_lon = gate_point['latitude'], gate_point['longitude']\n",
    "    \n",
    "    # Calculate distances from the gate_point to all points in the GeoDataFrame\n",
    "    distances = gdf.apply(lambda row: geodesic((gate_lat, gate_lon), (row['latitude'], row['longitude'])).meters, axis=1)\n",
    "\n",
    "    # Find the index of the closest point\n",
    "    closest_index = distances.idxmin()\n",
    "\n",
    "\n",
    "    # Find the second & third closest point (before and after the closest)\n",
    "    if closest_index == 0:\n",
    "        closest_index = 1\n",
    "        second_closest_index = 0\n",
    "        third_closest_index = 2\n",
    "    elif closest_index == len(gdf) - 1:\n",
    "        closest_index = len(gdf) - 2\n",
    "        second_closest_index = len(gdf) - 3\n",
    "        third_closest_index = -1\n",
    "    else:\n",
    "        second_closest_index = closest_index - 1\n",
    "        third_closest_index = closest_index + 1\n",
    "    \n",
    "    return closest_index, second_closest_index, third_closest_index\n",
    "\n",
    "\n",
    "\n",
    "def interpolate_between_points(gate_point, gdf, idx1, idx2, idx3):\n",
    "    \"\"\"\n",
    "    Interpolate the closest point on the geodesic line (great circle) between two track points to the gate point.\n",
    "    Uses PyGeodesy to calculate the closest point along the geodesic line.\n",
    "    \"\"\"\n",
    "    # Extract the two closest points from the GeoDataFrame\n",
    "    point1 = gdf.iloc[idx1]\n",
    "    point2 = gdf.iloc[idx2]\n",
    "    point3 = gdf.iloc[idx3]\n",
    "    \n",
    "    # Use PyGeodesy to find the nearest point on the geodesic between point1 and point2\n",
    "    #nearest = gate_geo.nearestOn3(point1_geo, point2_geo)\n",
    "    \n",
    "    # Create PyGeodesy LatLon objects for the track points and gate point\n",
    "    point1_geo = st.LatLon(point1['latitude'], point1['longitude'])\n",
    "    point2_geo = st.LatLon(point2['latitude'], point2['longitude'])\n",
    "    point3_geo = st.LatLon(point3['latitude'], point3['longitude'])\n",
    "    gate_geo = st.LatLon(gate_point['latitude'], gate_point['longitude'])\n",
    "\n",
    "    # Use PyGeodesy to find the nearest point on the geodesic between point1 and point2\n",
    "    #nearest = gate_geo.nearestOn(point1_geo, point2_geo, point3_geo)\n",
    "    #print(nearest)\n",
    "    nearest = [st.nearestOn3(gate_geo, [point1_geo,point2_geo,point3_geo])] \n",
    "    nearest_geo = nearest[0].closest\n",
    "    \n",
    "    # Interpolate time between the two points\n",
    "    time_diff = (point3['time'] - point2['time']).total_seconds()\n",
    "    dist_diff = point3_geo.distanceTo(point2_geo)\n",
    "    dist_to = point2_geo.distanceTo(nearest_geo)\n",
    "    fraction_of_distance = dist_to/dist_diff\n",
    "    interpolated_time = point2['time'] + pd.Timedelta(seconds=time_diff * fraction_of_distance)\n",
    "\n",
    "    # Compute distance from the gate point to the nearest point on the geodesic\n",
    "    distance_from_gate = gate_geo.distanceTo(nearest_geo)\n",
    "\n",
    "    # Return the results in a dictionary\n",
    "    return {\n",
    "        'gate_lat': gate_point['latitude'],\n",
    "        'gate_lon': gate_point['longitude'],\n",
    "        'interpolated_lat': nearest_geo.lat,\n",
    "        'interpolated_lon': nearest_geo.lon,\n",
    "        'interpolated_time': interpolated_time,\n",
    "        'distance_from_gate_m': distance_from_gate\n",
    "    }\n",
    "\n",
    "\n",
    "# Function to process each gate point and interpolate\n",
    "def compute_track_points_for_gate(gate_points, gdf, map_obj):\n",
    "    interpolated_results = []\n",
    "\n",
    "    for gate_point in gate_points:\n",
    "        idx1, idx2, idx3 = find_closest_points(gate_point, gdf)\n",
    "        result = interpolate_between_points(gate_point, gdf, idx1, idx2, idx3)\n",
    "\n",
    "        # Add interpolated point to the map (red marker)\n",
    "        folium.Marker(\n",
    "            location=[result['interpolated_lat'], result['interpolated_lon']],\n",
    "            popup=(\n",
    "                f\"Interpolated Point<br>Coordinates: ({result['interpolated_lat']}, {result['interpolated_lon']})<br>\"\n",
    "                f\"Time: {result['interpolated_time']}<br>\"\n",
    "                f\"Distance from gate: {result['distance_from_gate_m']} meters\"\n",
    "            ),\n",
    "            icon=folium.Icon(color='red')\n",
    "        ).add_to(map_obj)\n",
    "\n",
    "        interpolated_results.append(result)\n",
    "\n",
    "    return pd.DataFrame(interpolated_results)\n",
    "\n",
    "# Function to check if a name starts with \"in\" or \"out\" followed by a number\n",
    "def is_in_or_out_with_number(name):\n",
    "    # Use regex to match \"in\" or \"out\" followed by a number\n",
    "    return bool(re.match(r'^(in|out)\\d+', name.lower()))\n",
    "\n",
    "# Extract the gate points from the route data where name matches the pattern\n",
    "gate_points = []\n",
    "for index, row in route_df.iterrows():\n",
    "    if is_in_or_out_with_number(row['name']):\n",
    "        # Create a dictionary with the latitude, longitude, and elevation\n",
    "        gate_point = {\n",
    "            'latitude': row['latitude'],\n",
    "            'longitude': row['longitude'],\n",
    "            'elevation': row['altitude']  # Assuming 'altitude' is the correct field for elevation\n",
    "        }\n",
    "        # Add the gate point to the list\n",
    "        gate_points.append(gate_point)\n",
    "\n",
    "# Assuming gdf is already created with track points\n",
    "# Optionally: Save the updated GeoDataFrame to a file or visualize using Folium\n",
    "\n",
    "\n",
    "# Create a Folium map centered on the first point\n",
    "m = folium.Map(location=[gdf.iloc[0]['latitude'], gdf.iloc[0]['longitude']], zoom_start=10)\n",
    "\n",
    "# Function to check if a name starts with \"in\" or \"out\" followed by a number\n",
    "def is_in_or_out_with_number(name):\n",
    "    # Use regex to match \"in\" or \"out\" followed by a number\n",
    "    return bool(re.match(r'^(in|out)\\d+', name.lower()))\n",
    "\n",
    "# Plot each route point\n",
    "for index, row in route_df.iterrows():\n",
    "    # Determine the icon color: red for points with names starting with \"in\" or \"out\" followed by a number, otherwise orange\n",
    "    icon_color = 'green' if is_in_or_out_with_number(row['name']) else 'orange'\n",
    "    \n",
    "    folium.Marker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        popup=(f\"Name: {row['name']}<br>Type: {row['type']}<br>Altitude: {row['altitude']} ft<br>Description: {row['description']}\"),\n",
    "        icon=folium.Icon(color=icon_color, icon='info-sign')\n",
    "    ).add_to(m)\n",
    "    \n",
    "# Add the GPS track to the map\n",
    "for i, row in gdf.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        popup=(\n",
    "            f\"Id: {i} <pr>\"\n",
    "            f\"Elevation: {row['elevation_ft']} ft\\n\"\n",
    "            f\"Speed Av: {row['speed_avplan'] * MPS_TO_KNOTS} knots\\n\"\n",
    "            f\"Speed Cal: {row['speed_calc'] * MPS_TO_KNOTS} knots\\n\"\n",
    "            f\"Heading: {row['heading']}°\\n\"\n",
    "            f\"Distance: {row['distance_m']} m\\n\"\n",
    "            f\"Interval: {row['time_diff_sec']}\\n\"\n",
    "            f\"Elevation change: {row['elevation_change_ft']} ft\\n\"\n",
    "            f\"Angle: {row['angle_deg']}°\\n\"\n",
    "            f\"Time: {row['time']}\")\n",
    "    ).add_to(m)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "m.save('map_with_gpx_data.html')\n",
    "\n",
    "# Display the map (if running in a Jupyter notebook)\n",
    "m  # Uncomment if running in Jupyter\n",
    "\n",
    "\n",
    "# Call the function to compute track points and plot them on the map\n",
    "interpolated_gate_points = compute_track_points_for_gate(gate_points, gdf, m)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "m.save('map_with_gates_and_interpolated_points.html')\n",
    "\n",
    "# Display the resulting DataFrame with interpolated data\n",
    "print(interpolated_gate_points['distance_from_gate_m'])\n",
    "print(interpolated_gate_points['interpolated_time'].diff())\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c20d85c7-0614-43c3-ab7f-e375d28f62b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No file selected\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "def select_file():\n",
    "    # Initialize the Tkinter root window\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the root window\n",
    "    \n",
    "    # Open the file selection dialog\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Select a file\",\n",
    "        filetypes=[(\"All Files\", \"*.*\"), (\"Text Files\", \"*.txt\"), (\"Python Files\", \"*.py\")]\n",
    "    )\n",
    "    \n",
    "    # Print the selected file path\n",
    "    if file_path:\n",
    "        print(f\"Selected file: {file_path}\")\n",
    "    else:\n",
    "        print(\"No file selected\")\n",
    "\n",
    "# Call the file selection dialog\n",
    "select_file()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "809a8107-7feb-4f7e-9773-87c5cfcc87b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for first and last record, please wait\n",
      "ffffffffffffffffffffffffffffffffffffffffffffffbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbrp_EOF_i1rp_nonefl_breakOptional record sections present:\n",
      " RDAC1\n",
      " Attitude\n",
      " GPS\n",
      "Dumping packets from:0 please wait\n",
      "ppppppppppppppppppppppppppppppppppppppppppppppbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbrp_EOF_i1rp_none                        Time   alt  baro ASI TAS   VSI    g-s rotor   m-v  \\\n",
      "0      \"2018/03/23 08:48:22\"  1695  3123  32  32     4   99.9     0  13.1   \n",
      "1      \"2018/03/23 08:48:23\"  1697  3123  39  40   309   11.3     0  13.1   \n",
      "2      \"2018/03/23 08:48:24\"  1696  3123  44  45  -111  -35.6     0  13.1   \n",
      "3      \"2018/03/23 08:48:25\"  1696  3123  49  50    53   83.0     0  13.2   \n",
      "4      \"2018/03/23 08:48:26\"  1695  3123  53  54   -39  -99.9     0  13.2   \n",
      "...                      ...   ...   ...  ..  ..   ...    ...   ...   ...   \n",
      "46295  \"2018/06/05 14:59:30\"  1427  3157   0   0   -90    0.0     0  12.2   \n",
      "46296  \"2018/06/05 14:59:31\"  1429  3157   0   0    82    0.0     0  12.1   \n",
      "46297  \"2018/06/05 14:59:32\"  1428  3157   0   0  -101    0.0     0  12.1   \n",
      "46298  \"2018/06/05 14:59:33\"  1431  3157   0   0   170    0.0     0  12.2   \n",
      "46299  \"2018/06/05 14:59:34\"  1430  3157   0   0   -69    0.0     0  12.2   \n",
      "\n",
      "        b-v  ...    turn        lat       long   hdg  gs g-alt status sats  \\\n",
      "0      12.6  ...   -4:00  -35.30131  149.18610  1319  33  1883      3   10   \n",
      "1      12.5  ...  -14:00  -35.30141  149.18623  1308  36  1883      3   11   \n",
      "2      12.5  ...   -4:00  -35.30151  149.18639  1295  40  1883      3   11   \n",
      "3      12.4  ...   -4:00  -35.30162  149.18654  1296  44  1883      3   11   \n",
      "4      12.4  ...    1:00  -35.30174  149.18671  1296  47  1883      3   11   \n",
      "...     ...  ...     ...        ...        ...   ...  ..   ...    ...  ...   \n",
      "46295  11.6  ...    3:00  -35.30435  149.19048  3108  24  1868      3   13   \n",
      "46296  11.6  ...    1:00  -35.30429  149.19038  3115  22  1868      3   13   \n",
      "46297  11.6  ...  -30:00  -35.30426  149.19035  3114  20  1868      3   13   \n",
      "46298  11.6  ...  -63:00  -35.30418  149.19023  3036  20  1868      3   13   \n",
      "46299  11.6  ...  -70:00  -35.30415  149.19014  2964  19  1868      3   12   \n",
      "\n",
      "      hac vac  \n",
      "0       5   7  \n",
      "1       5   7  \n",
      "2       5   7  \n",
      "3       5   7  \n",
      "4       5   7  \n",
      "...    ..  ..  \n",
      "46295   5   7  \n",
      "46296   5   7  \n",
      "46297   5   7  \n",
      "46298   5   7  \n",
      "46299   5   7  \n",
      "\n",
      "[46300 rows x 62 columns]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import struct\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "# Constants and Definitions\n",
    "HEADER_LEN = 8\n",
    "SIGNATURE_LEN = 2\n",
    "PRIMARY_FLIGHT_DATA_LEN = 24\n",
    "RDAC1_DATA_LEN = 56\n",
    "RDAC1_DATA_LEN_REV_2 = 63\n",
    "RDAC2_DATA_LEN = RDAC1_DATA_LEN\n",
    "RDAC2_DATA_LEN_REV_2 = RDAC1_DATA_LEN_REV_2\n",
    "ATTITUDE_DATA_LEN = 14\n",
    "GPS_DATA_LEN_REV_0 = 20\n",
    "GPS_DATA_LEN_REV_1 = 24\n",
    "\n",
    "MAX_PACKET = 1000  # larger than any legal packet\n",
    "\n",
    "PACKET_SIGNATURE_1 = 0xaa\n",
    "PACKET_SIGNATURE_2 = 0x55\n",
    "SIGNATURE_LEN = 2\n",
    "\n",
    "TIMESTAMP_LEN = 4\n",
    "\n",
    "MAX_REVISION = 3  # counting from zero\n",
    "\n",
    "delimiters = {\n",
    "    \"tab\": \"\\t\",\n",
    "    \"comma\": \",\",\n",
    "    \"space\": \" \",\n",
    "    \"no_delim\": \"\",\n",
    "}\n",
    "\n",
    "fudge_factors = [6, 2, 2, 6]  # length is MAX_REVISION+1 = 4\n",
    "\n",
    "primary_section = 0\n",
    "rdac1_section = 1\n",
    "rdac2_section = 2\n",
    "attitude_section = 3\n",
    "gps_section = 4\n",
    "\n",
    "section_sizes = [\n",
    "    [24, 24, 24, 24],  # primary\n",
    "    [56, 56, 63, 63],  # rdac1\n",
    "    [56, 56, 63, 63],  # rdac2\n",
    "    [14, 14, 14, 14],  # attitude\n",
    "    [20, 24, 24, 24],  # GPS\n",
    "]\n",
    "\n",
    "rdac1_data_tag = 1\n",
    "rdac2_data_tag = 2\n",
    "attitude_data_tag = 3\n",
    "gps_data_tag = 4\n",
    "\n",
    "RDAC1_PRESENT = 1 << rdac1_data_tag  # (1 << 1) == 2\n",
    "RDAC2_PRESENT = 1 << rdac2_data_tag  # (1 << 2) == 4\n",
    "ATTITUDE_PRESENT = 1 << attitude_data_tag  # (1 << 3) == 8\n",
    "GPS_PRESENT = 1 << gps_data_tag  # (1 << 4) == 16\n",
    "\n",
    "\n",
    "class EnigmaLogParser:\n",
    "    def __init__(self, filename):\n",
    "        self.logfile = open(filename, 'rb')\n",
    "        self.bad_packets = 0\n",
    "        self.reversals = 0\n",
    "\n",
    "    def read_packet(self):\n",
    "        packet = bytearray()\n",
    "        processed = 0\n",
    "        while True:\n",
    "            i1 = self.logfile.read(1)\n",
    "            if not i1:\n",
    "                print(\"rp_EOF_i1\", end='')    \n",
    "                return None  # EOF\n",
    "            if i1[0] == PACKET_SIGNATURE_1:\n",
    "                i2 = self.logfile.read(1)\n",
    "                if not i2:\n",
    "                    print(\"rp_EOF_i2\", end='')      \n",
    "                    return None  # EOF\n",
    "                if i2[0] == PACKET_SIGNATURE_2:\n",
    "                    # Valid starting signature, read rest of packet\n",
    "                    packet_body = self.read_body()\n",
    "                    if packet_body is None:\n",
    "                        return None\n",
    "                    packet.extend(i1)\n",
    "                    packet.extend(i2)\n",
    "                    packet.extend(packet_body)\n",
    "                    return packet\n",
    "                else:\n",
    "                    # Move back one byte\n",
    "                    self.logfile.seek(-1, 1)\n",
    "            processed += 1\n",
    "            if processed % 100 == 0:\n",
    "                print(\"rp\", end='')\n",
    "            \n",
    "        return None  # Should not reach here\n",
    "\n",
    "    def read_body(self):\n",
    "        packet_body = bytearray()\n",
    "        processed = 0\n",
    "        while True:\n",
    "            i1 = self.logfile.read(1)\n",
    "            if not i1:\n",
    "                break  # EOF\n",
    "            if i1[0] == PACKET_SIGNATURE_1:\n",
    "                i2 = self.logfile.read(1)\n",
    "                if not i2:\n",
    "                    break  # EOF\n",
    "                if i2[0] == PACKET_SIGNATURE_2:\n",
    "                    # Signature of next packet, back up and exit\n",
    "                    self.logfile.seek(-2, 1)\n",
    "                    break\n",
    "                else:\n",
    "                    # False trigger, save byte and continue\n",
    "                    packet_body.append(i1[0])\n",
    "                    self.logfile.seek(-1, 1)\n",
    "            else:\n",
    "                packet_body.append(i1[0])\n",
    "                \n",
    "            processed += 1\n",
    "            if processed % 1000000 == 0:\n",
    "                print(f'b', end='')\n",
    "                \n",
    "  \n",
    "        return packet_body\n",
    "\n",
    "    def read_and_id_packet(self):\n",
    "        while True:\n",
    "            packet = self.read_packet()\n",
    "            if packet is None:\n",
    "                print(\"rp_none\", end='')\n",
    "            \n",
    "                return None\n",
    "            packet_len = len(packet)\n",
    "            if packet_len < HEADER_LEN:\n",
    "                continue  # Try next packet\n",
    "\n",
    "            timestamp = (\n",
    "                packet[7] << 24 |\n",
    "                packet[6] << 16 |\n",
    "                packet[5] << 8 |\n",
    "                packet[4]\n",
    "            )\n",
    "            # Extract bytes from index 4 to 7 (inclusive)\n",
    "            bytes_4 = packet[3:7]  # Slices from index 3 to 6\n",
    "            # Convert the extracted 4 bytes to an integer (big-endian or little-endian)\n",
    "            #timestamp = int.from_bytes(bytes_4, byteorder='little') # 'big' or 'little'\n",
    "            \n",
    "            payload = packet\n",
    "            # packet[0] is the total payload length + fudge factor\n",
    "            # packet[1] is an offset\n",
    "\n",
    "            # Convert each byte to its hexadecimal representation and join them with a space\n",
    "            hex_output = ' '.join(f'{byte:02X}' for byte in packet)\n",
    "\n",
    "            # Print the result\n",
    "#            print(f'Timestamp: {timestamp} for Packet:{hex_output}')  \n",
    "            for revision in range(MAX_REVISION + 1):\n",
    "                sections_seen = 0\n",
    "                rdac1_len = 0\n",
    "                rdac2_len = 0\n",
    "                attitude_len = 0\n",
    "                gps_len = 0\n",
    "\n",
    "                if revision >= len(fudge_factors):\n",
    "                    continue\n",
    "\n",
    "                if packet[0] < fudge_factors[revision]:\n",
    "                    continue\n",
    "\n",
    "                payload_len = packet[2] + SIGNATURE_LEN\n",
    "\n",
    "                #if (payload_len < PRIMARY_FLIGHT_DATA_LEN or\n",
    "                #        payload_len + HEADER_LEN - SIGNATURE_LEN != packet_len):\n",
    "                #    continue\n",
    "\n",
    "                p_offset = section_sizes[0][revision]+ HEADER_LEN + fudge_factors[revision]\n",
    "                if p_offset < 0 or p_offset > len(payload):\n",
    "                    continue  # Invalid offset\n",
    "\n",
    "                p_idx = p_offset\n",
    "\n",
    "                # Parse sections\n",
    "                if p_idx < payload_len and payload[p_idx] == rdac1_data_tag:\n",
    "                    sections_seen |= RDAC1_PRESENT\n",
    "                    tag = payload[p_idx]\n",
    "                    p_idx += 1\n",
    "                    rdac1_len = payload[p_idx]\n",
    "                    p_idx += 1\n",
    "                    p_idx += rdac1_len\n",
    "\n",
    "                if p_idx < payload_len and payload[p_idx] == rdac2_data_tag:\n",
    "                    sections_seen |= RDAC2_PRESENT\n",
    "                    tag = payload[p_idx]\n",
    "                    p_idx += 1\n",
    "                    rdac2_len = payload[p_idx]\n",
    "                    p_idx += 1\n",
    "                    p_idx += rdac2_len\n",
    "\n",
    "                if p_idx < payload_len and payload[p_idx] == attitude_data_tag:\n",
    "                    sections_seen |= ATTITUDE_PRESENT\n",
    "                    tag = payload[p_idx]\n",
    "                    p_idx += 1\n",
    "                    attitude_len = payload[p_idx]\n",
    "                    p_idx += 1\n",
    "                    p_idx += attitude_len\n",
    "\n",
    "                if p_idx < payload_len and payload[p_idx] == gps_data_tag:\n",
    "                    sections_seen |= GPS_PRESENT\n",
    "                    tag = payload[p_idx]\n",
    "                    p_idx += 1\n",
    "                    gps_len = payload[p_idx]\n",
    "                    p_idx += 1\n",
    "                    p_idx += gps_len\n",
    "\n",
    "                if (p_idx == payload_len and\n",
    "                    ((sections_seen & RDAC1_PRESENT == 0) or rdac1_len == section_sizes[rdac1_section][revision]) and\n",
    "                    ((sections_seen & RDAC2_PRESENT == 0) or rdac2_len == section_sizes[rdac2_section][revision]) and\n",
    "                    ((sections_seen & ATTITUDE_PRESENT == 0) or attitude_len == section_sizes[attitude_section][revision]) and\n",
    "                    ((sections_seen & GPS_PRESENT == 0) or gps_len == section_sizes[gps_section][revision])\n",
    "                   ):\n",
    "                    #print(payload, payload_len, revision, sections_seen)\n",
    "                    return timestamp, payload[HEADER_LEN:], payload_len, revision, sections_seen\n",
    "\n",
    "            self.bad_packets += 1\n",
    "            # Try next packet\n",
    "\n",
    "    def find_first_last(self):\n",
    "        oldest_pos = -1\n",
    "        newest_pos = -1\n",
    "        sections_seen = 0\n",
    "        reversals = 0\n",
    "\n",
    "        self.logfile.seek(0, 0)\n",
    "        cur_pos = self.logfile.tell()\n",
    "        oldest = 4294967295 # For 32-bit unsigned long\n",
    "        newest = 0\n",
    "        last = 0\n",
    "\n",
    "        processed = 0\n",
    "\n",
    "        while True:\n",
    "            pos = self.logfile.tell()\n",
    "            result = self.read_and_id_packet()\n",
    "            #print(\"fl_it\", end='')\n",
    "            if result is None:\n",
    "                print(\"fl_break\", end='')\n",
    "            \n",
    "                break\n",
    "            timestamp, payload, payload_len, revision, one_packet_sections_seen = result\n",
    "            if processed:\n",
    "                if sections_seen != one_packet_sections_seen:\n",
    "                    print(\"\\nWarning, records contain inconsistent sections\\n\")\n",
    "                \n",
    "            else:\n",
    "                pass  # First packet\n",
    "\n",
    "            sections_seen |= one_packet_sections_seen\n",
    "            #print(f'Timestamp: {timestamp}')\n",
    "        \n",
    "            if timestamp < oldest:\n",
    "                oldest = timestamp\n",
    "                oldest_pos = pos\n",
    "\n",
    "            if timestamp > newest:\n",
    "                newest = timestamp\n",
    "                newest_pos = pos\n",
    "\n",
    "            if timestamp < last:\n",
    "                reversals += 1\n",
    "\n",
    "            last = timestamp\n",
    "\n",
    "            processed += 1\n",
    "            if processed % 1000 == 0:\n",
    "                print(\"f\", end='')\n",
    "            \n",
    "\n",
    "        return oldest_pos, newest_pos, sections_seen, reversals\n",
    "\n",
    "    def rdac_headers(self):\n",
    "        headers = [\"RPM\", \"rfl1\", \"rfl2\", \"ch1\", \"ch2\", \"flow\", \"MAP\", \"fl1\", \"fl2\", \"flc\",\n",
    "                   \"o-t\", \"oil-p\", \"carb\", \"f-p\", \"h2o\"]\n",
    "        headers.extend([f\"tc{i}\" for i in range(1, 13)])\n",
    "        headers.extend([\"rch1\", \"rch2\", \"rot\", \"rop\", \"ref\", \"fail\"])\n",
    "        return headers\n",
    "\n",
    "    def print_header(self, delimiter_type, sections_seen):\n",
    "        delimiter = delimiters[delimiter_type]\n",
    "        headers = []\n",
    "        headers.append(\"Time\")\n",
    "\n",
    "        headers.extend([\"alt\", \"baro\", \"ASI\", \"TAS\", \"VSI\", \"g-s\", \"rotor\", \"m-v\", \"b-v\", \"amps\", \"AOA\", \"OAT\"])\n",
    "\n",
    "        if sections_seen & RDAC1_PRESENT:\n",
    "            headers.extend(self.rdac_headers())\n",
    "        if sections_seen & RDAC2_PRESENT:\n",
    "            headers.extend(self.rdac_headers())\n",
    "        if sections_seen & ATTITUDE_PRESENT:\n",
    "            headers.extend([\"bank\", \"ptch\", \"slip\", \"hdg\", \"yaw\", \"G\", \"turn\"])\n",
    "        if sections_seen & GPS_PRESENT:\n",
    "            headers.extend([\"lat\", \"long\", \"hdg\", \"gs\", \"g-alt\", \"status\", \"sats\", \"hac\", \"vac\"])\n",
    "\n",
    "        #print(delimiter.join(headers))\n",
    "        file.write(delimiter.join(headers) + \"\\n\")  # Write headers to the file\n",
    "        return headers\n",
    "\n",
    "\n",
    "    def process_packets(self, delimiter_type, stop_pos, sections_seen):\n",
    "        delimiter = delimiters[delimiter_type]\n",
    "        packets = 0\n",
    "        mgl_records = []\n",
    "        while self.logfile.tell() < stop_pos:\n",
    "            result = self.read_and_id_packet()\n",
    "            if result is None:\n",
    "                break\n",
    "            timestamp, payload, payload_len, revision, packet_sections_seen = result\n",
    "            p = payload\n",
    "            idx = 0\n",
    "            # Set the block size (number of bytes per line)\n",
    "            block_size = 10\n",
    "            # Convert each byte to its hexadecimal representation and join them with a space\n",
    "            hex_output = ' '.join(f'{byte:02X}' for byte in p)\n",
    "            # Split the hex output into blocks of 'block_size' and print each block on a new line\n",
    "            hex_blocks = hex_output.split()  # Convert hex string to a list of hex values\n",
    "            #print(f'{len(p)} ', end='')\n",
    "            #for i in range(0, len(hex_blocks), block_size):\n",
    "            #    # Print each block (8 hex values per line)\n",
    "            #    print(' '.join(hex_blocks[i:i + block_size]))\n",
    "            valid = True\n",
    "            #print(f'T:{timestamp} - Len:{payload_len} - Rev:{revision}')\n",
    "            # Time processing\n",
    "            if revision >= 2:\n",
    "                base_time = datetime.datetime(2000, 1, 1)\n",
    "                adj_time = base_time + datetime.timedelta(seconds=timestamp)\n",
    "                time_str = adj_time.strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "                output = [f'\"{time_str}\"']\n",
    "            else:\n",
    "                output = [str(timestamp)]\n",
    "\n",
    "            # Primary Flight Data\n",
    "            # Using struct.unpack for binary data parsing\n",
    "            try:\n",
    "                # Altitude (S32)\n",
    "                alt, = struct.unpack_from('<i', p, idx)\n",
    "                idx += 4\n",
    "                output.append(str(alt))\n",
    "                # Barometer (S16)\n",
    "                baro, = struct.unpack_from('<h', p, idx)\n",
    "                idx += 2\n",
    "                output.append(str(baro))\n",
    "                # Airspeed (S16)\n",
    "                asi, = struct.unpack_from('<h', p, idx)\n",
    "                idx += 2\n",
    "                output.append(str(asi))\n",
    "                # TAS (S16)\n",
    "                tas, = struct.unpack_from('<h', p, idx)\n",
    "                idx += 2\n",
    "                output.append(str(tas))\n",
    "                # VSI (S16)\n",
    "                vsi, = struct.unpack_from('<h', p, idx)\n",
    "                idx += 2\n",
    "                output.append(str(vsi))\n",
    "                # Glide Slope (S16 tenths)\n",
    "                gs, = struct.unpack_from('<h', p, idx)\n",
    "                idx += 2\n",
    "                gs_tenths = gs / 10.0\n",
    "                output.append(f\"{gs_tenths:.1f}\")\n",
    "                # Rotor RPM (U16)\n",
    "                rotor_raw, = struct.unpack_from('<H', p, idx)\n",
    "                idx += 2\n",
    "                rotor = rotor_raw & 0x7fff\n",
    "                output.append(str(rotor))\n",
    "                # Main Voltage (U8 tenths)\n",
    "                mv_raw, = struct.unpack_from('<B', p, idx)\n",
    "                idx += 1\n",
    "                mv = mv_raw / 10.0\n",
    "                output.append(f\"{mv:.1f}\")\n",
    "                # Backup Voltage (U8 tenths)\n",
    "                bv_raw, = struct.unpack_from('<B', p, idx)\n",
    "                idx += 1\n",
    "                bv = bv_raw / 10.0\n",
    "                output.append(f\"{bv:.1f}\")\n",
    "                # Current (S16 tenths)\n",
    "                amps_raw, = struct.unpack_from('<h', p, idx)\n",
    "                idx += 2\n",
    "                amps = amps_raw / 10.0\n",
    "                output.append(f\"{amps:.1f}\")\n",
    "                # AOA (S16)\n",
    "                aoa, = struct.unpack_from('<h', p, idx)\n",
    "                idx += 2\n",
    "                output.append(str(aoa))\n",
    "                # OAT (S16)\n",
    "                oat, = struct.unpack_from('<h', p, idx)\n",
    "                idx += 2\n",
    "                output.append(str(oat))\n",
    "            except struct.error:\n",
    "                self.bad_packets += 1\n",
    "                continue  # Skip to next packet\n",
    "            idx += fudge_factors[revision] #Seem to need to skip some extra bytes here\n",
    "            # RDAC Sections\n",
    "            for rdac_tag in [rdac1_data_tag, rdac2_data_tag]:\n",
    "                if sections_seen & (1 << rdac_tag):\n",
    "                    if idx < len(p) and p[idx] == rdac_tag:\n",
    "                        # Unpack tag and length\n",
    "                        tag, length = struct.unpack_from('<BB', p, idx)\n",
    "                        idx += 2  # Move index past tag and length\n",
    "\n",
    "                        # Direct use of struct.unpack_from for each data field\n",
    "                        # RPM (16-bit unsigned)\n",
    "                        rpm = struct.unpack_from('<H', p, idx)[0] if valid else 0\n",
    "                        idx += 2\n",
    "                        output.append(str(rpm))\n",
    "\n",
    "                        # Tank 1 raw (16-bit unsigned)\n",
    "                        tank1_raw = struct.unpack_from('<H', p, idx)[0] if valid else 0\n",
    "                        idx += 2\n",
    "                        output.append(str(tank1_raw))\n",
    "\n",
    "                        # Tank 2 raw (16-bit unsigned)\n",
    "                        tank2_raw = struct.unpack_from('<H', p, idx)[0] if valid else 0\n",
    "                        idx += 2\n",
    "                        output.append(str(tank2_raw))\n",
    "\n",
    "                        # Rotax CHT 1 (16-bit unsigned)\n",
    "                        rotax_cht1 = struct.unpack_from('<H', p, idx)[0] if valid else 0\n",
    "                        idx += 2\n",
    "                        output.append(str(rotax_cht1))\n",
    "\n",
    "                        # Rotax CHT 2 (16-bit unsigned)\n",
    "                        rotax_cht2 = struct.unpack_from('<H', p, idx)[0] if valid else 0\n",
    "                        idx += 2\n",
    "                        output.append(str(rotax_cht2))\n",
    "\n",
    "                        # Fuel flow (16-bit unsigned, divided by 100)\n",
    "                        fuel_flow = struct.unpack_from('<H', p, idx)[0] / 100.0 if valid else 0.0\n",
    "                        idx += 2\n",
    "                        output.append(f\"{fuel_flow:.2f}\")\n",
    "\n",
    "                        # MAP (Manifold Absolute Pressure, 16-bit unsigned)\n",
    "                        map_val = struct.unpack_from('<H', p, idx)[0] if valid else 0\n",
    "                        idx += 2\n",
    "                        output.append(str(map_val))\n",
    "\n",
    "                        # Fuel level 1 (16-bit unsigned)\n",
    "                        fuel_level1 = struct.unpack_from('<H', p, idx)[0] if valid else 0\n",
    "                        idx += 2\n",
    "                        output.append(str(fuel_level1))\n",
    "\n",
    "                        # Fuel level 2 (16-bit unsigned)\n",
    "                        fuel_level2 = struct.unpack_from('<H', p, idx)[0] if valid else 0\n",
    "                        idx += 2\n",
    "                        output.append(str(fuel_level2))\n",
    "\n",
    "                        # Calculated fuel level (16-bit unsigned, divided by 10)\n",
    "                        calc_fuel_level = struct.unpack_from('<H', p, idx)[0] / 10.0 if valid else 0.0\n",
    "                        idx += 2\n",
    "                        output.append(f\"{calc_fuel_level:.1f}\")\n",
    "\n",
    "                        # Oil temperature (16-bit unsigned)\n",
    "                        oil_temp = struct.unpack_from('<H', p, idx)[0] if valid else 0\n",
    "                        idx += 2\n",
    "                        output.append(str(oil_temp))\n",
    "\n",
    "                        # Oil pressure (16-bit unsigned, divided by 10)\n",
    "                        oil_pressure = struct.unpack_from('<H', p, idx)[0] / 10.0 if valid else 0.0\n",
    "                        idx += 2\n",
    "                        output.append(f\"{oil_pressure:.1f}\")\n",
    "\n",
    "                        # Carb temperature (16-bit signed)\n",
    "                        carb_temp = struct.unpack_from('<h', p, idx)[0] if valid else 0\n",
    "                        idx += 2\n",
    "                        output.append(str(carb_temp))\n",
    "\n",
    "                        # Fuel pressure (8-bit unsigned, divided by 10)\n",
    "                        fuel_pressure = struct.unpack_from('<B', p, idx)[0] / 10.0 if valid else 0.0\n",
    "                        idx += 1\n",
    "                        output.append(f\"{fuel_pressure:.1f}\")\n",
    "\n",
    "                        # Water temperature (8-bit unsigned)\n",
    "                        water_temp = struct.unpack_from('<B', p, idx)[0] if valid else 0\n",
    "                        idx += 1\n",
    "                        output.append(str(water_temp))\n",
    "\n",
    "                        # Process 12 thermocouples (each 16-bit unsigned)\n",
    "                        for i in range(12):\n",
    "                            thermocouple = struct.unpack_from('<H', p, idx)[0] if valid else 0\n",
    "                            idx += 2\n",
    "                            output.append(str(thermocouple))\n",
    "\n",
    "                        # Additional fields if revision == 2\n",
    "                        if revision == 2:\n",
    "                            raw_cht1 = struct.unpack_from('<H', p, idx)[0] if valid else 0\n",
    "                            idx += 2\n",
    "                            output.append(str(raw_cht1))\n",
    "\n",
    "                            raw_cht2 = struct.unpack_from('<H', p, idx)[0] if valid else 0\n",
    "                            idx += 2\n",
    "                            output.append(str(raw_cht2))\n",
    "\n",
    "                            raw_oil_temp = struct.unpack_from('<H', p, idx)[0] if valid else 0\n",
    "                            idx += 2\n",
    "                            output.append(str(raw_oil_temp))\n",
    "\n",
    "                            raw_oil_pressure = struct.unpack_from('<H', p, idx)[0] if valid else 0\n",
    "                            idx += 2\n",
    "                            output.append(str(raw_oil_pressure))\n",
    "\n",
    "                            rdac_temp = struct.unpack_from('<H', p, idx)[0] if valid else 0\n",
    "                            #Result Doesn't compare to the MGL program e.g 2403 vs 21. Reads the bits correctly.\n",
    "                            idx += 2\n",
    "                            output.append(str(rdac_temp))\n",
    "\n",
    "                            rdac_fail = struct.unpack_from('<B', p, idx)[0] if valid else 0\n",
    "                            idx += 1\n",
    "                            output.append(str(rdac_fail))\n",
    "\n",
    "                        else:\n",
    "                            # If revision is not 2, treat raw CHT and oil fields as invalid (set valid=False)\n",
    "                            raw_cht1 = struct.unpack_from('<H', p, idx)[0] if False else 0\n",
    "                            idx += 2\n",
    "                            output.append(str(raw_cht1))\n",
    "\n",
    "                            raw_cht2 = struct.unpack_from('<H', p, idx)[0] if False else 0\n",
    "                            idx += 2\n",
    "                            output.append(str(raw_cht2))\n",
    "\n",
    "                            raw_oil_temp = struct.unpack_from('<H', p, idx)[0] if False else 0\n",
    "                            idx += 2\n",
    "                            output.append(str(raw_oil_temp))\n",
    "\n",
    "                            raw_oil_pressure = struct.unpack_from('<H', p, idx)[0] if False else 0\n",
    "                            idx += 2\n",
    "                            output.append(str(raw_oil_pressure))\n",
    "\n",
    "                            rdac_temp = struct.unpack_from('<H', p, idx)[0] if valid else 0\n",
    "                            idx += 2\n",
    "                            output.append(str(rdac_temp))\n",
    "\n",
    "                            rdac_fail = struct.unpack_from('<B', p, idx)[0] if valid else 0\n",
    "                            idx += 1\n",
    "                            output.append(str(rdac_fail))\n",
    "\n",
    "                            # Move past padding byte if present\n",
    "                            idx += 1\n",
    "                    else:\n",
    "                        output.extend(['3'] * 39)\n",
    "\n",
    "            # Attitude Section\n",
    "            if sections_seen & ATTITUDE_PRESENT:\n",
    "                if idx < len(p) and p[idx] == attitude_data_tag:\n",
    "                    # Unpack tag and length\n",
    "                    tag, length = struct.unpack_from('<BB', p, idx)\n",
    "                    idx += 2  # Move past tag and length\n",
    "\n",
    "                    # Bank angle (signed 16-bit integer)\n",
    "                    bank = struct.unpack_from('<h', p, idx)[0] if valid else 0\n",
    "                    idx += 2\n",
    "                    output.append(str(bank))\n",
    "\n",
    "                    # Pitch angle (signed 16-bit integer)\n",
    "                    pitch = struct.unpack_from('<h', p, idx)[0] if valid else 0\n",
    "                    idx += 2\n",
    "                    output.append(str(pitch))\n",
    "\n",
    "                    # Slip (signed 16-bit integer)\n",
    "                    slip = struct.unpack_from('<h', p, idx)[0] if valid else 0\n",
    "                    idx += 2\n",
    "                    output.append(str(slip))\n",
    "\n",
    "                    # Magnetic heading (signed 16-bit integer)\n",
    "                    magnetic_heading = struct.unpack_from('<h', p, idx)[0] if valid else 0\n",
    "                    idx += 2\n",
    "                    output.append(str(magnetic_heading))\n",
    "\n",
    "                    # Yaw/gyro heading (unsigned 16-bit integer)\n",
    "                    yaw_gyro_heading = struct.unpack_from('<H', p, idx)[0] if valid else 0\n",
    "                    idx += 2\n",
    "                    output.append(str(yaw_gyro_heading))\n",
    "\n",
    "                    # G-force (signed 8-bit integer, interpreted in tenths)\n",
    "                    g_force = struct.unpack_from('<b', p, idx)[0] / 10.0 if valid else 0.0\n",
    "                    idx += 1\n",
    "                    output.append(f\"{g_force:.1f}\")\n",
    "\n",
    "                    # Turn rate (Z-format MMSS: unsigned byte of seconds followed by signed word of minutes)\n",
    "                    turn_rate_seconds = struct.unpack_from('<B', p, idx)[0] if valid else 0  # unsigned 8-bit integer for seconds\n",
    "                    idx += 1\n",
    "                    turn_rate_minutes = struct.unpack_from('<h', p, idx)[0] if valid else 0  # signed 16-bit integer for minutes\n",
    "                    idx += 2\n",
    "\n",
    "                    # Format and append turn rate in MM:SS format\n",
    "                    turn_rate = f\"{turn_rate_minutes}:{turn_rate_seconds:02d}\"\n",
    "                    output.append(turn_rate)\n",
    "                    \n",
    "                else:\n",
    "                    output.extend(['7'] * 7)\n",
    "\n",
    "            # GPS Section\n",
    "            if sections_seen & GPS_PRESENT:\n",
    "                if idx < len(p) and p[idx] == gps_data_tag:          \n",
    "                    # Unpack GPS tag and length (both are unsigned 8-bit integers)\n",
    "                    tag, length = struct.unpack_from('<BB', p, idx)\n",
    "                    idx += 2  # Move index past tag and length\n",
    "\n",
    "                    # Interpret latitude as a 32-bit float (4 bytes)\n",
    "                    latitude, = struct.unpack_from('<f', p, idx)\n",
    "                    idx += 4\n",
    "                    output.append(f\"{latitude:.5f}\")  # Append latitude to output\n",
    "\n",
    "                    # Interpret longitude as a 32-bit float (4 bytes), negated (multiply by -1)\n",
    "                    longitude, = struct.unpack_from('<f', p, idx)\n",
    "                    idx += 4\n",
    "                    output.append(f\"{longitude:.5f}\")  # Append negated longitude\n",
    "\n",
    "                    # Interpret GPS heading as signed 32-bit integer\n",
    "                    heading, = struct.unpack_from('<i', p, idx)\n",
    "                    idx += 4\n",
    "                    output.append(str(heading))  # Append GPS heading to output\n",
    "\n",
    "                    # Interpret ground speed as signed 32-bit integer\n",
    "                    ground_speed, = struct.unpack_from('<i', p, idx)\n",
    "                    idx += 4\n",
    "                    output.append(str(ground_speed))  # Append ground speed to output\n",
    "\n",
    "                    # Interpret altitude as signed 32-bit integer\n",
    "                    altitude, = struct.unpack_from('<i', p, idx)\n",
    "                    idx += 4\n",
    "                    output.append(str(altitude))  # Append altitude to output\n",
    "\n",
    "                    # GPS status, satellites, horizontal accuracy, vertical accuracy are 8-bit integers\n",
    "                    if revision > 0:  # These fields are valid only for certain revisions\n",
    "                        gps_status, = struct.unpack_from('<b', p, idx)\n",
    "                        idx += 1\n",
    "                        output.append(str(gps_status))  # Append GPS status to output\n",
    "\n",
    "                        satellites, = struct.unpack_from('<b', p, idx)\n",
    "                        idx += 1\n",
    "                        output.append(str(satellites))  # Append satellites count to output\n",
    "\n",
    "                        horz_accuracy, = struct.unpack_from('<b', p, idx)\n",
    "                        idx += 1\n",
    "                        output.append(str(horz_accuracy))  # Append horizontal accuracy to output\n",
    "\n",
    "                        vert_accuracy, = struct.unpack_from('<b', p, idx)\n",
    "                        idx += 1\n",
    "                        output.append(str(vert_accuracy))  # Append vertical accuracy to output\n",
    "\n",
    "                else:\n",
    "                    output.extend(['9'] * 9)       \n",
    "            #print(delimiter.join(output))\n",
    "            #print(output)\n",
    "            file.write(delimiter.join(output) + \"\\n\")  # Write packet to the file\n",
    "            mgl_records.append(output)\n",
    "            packets += 1\n",
    "            if packets % 1000 == 0:\n",
    "                print(\"p\", end='')\n",
    "            \n",
    "        return mgl_records\n",
    "\n",
    "#Execution Starts Here\n",
    "\n",
    "delimiter_type = 'comma'\n",
    "filename =  '2018-6-27-Enigma' #'OAR-Enigma'\n",
    "fileext = 'rec'\n",
    "\n",
    "\n",
    "parser = EnigmaLogParser(f\"{filename}.{fileext}\")\n",
    "print(\"Scanning for first and last record, please wait\")\n",
    "\n",
    "\n",
    "    \n",
    "oldest_pos, newest_pos, sections_seen, reversals = parser.find_first_last()\n",
    "\n",
    "if oldest_pos == -1 or newest_pos == -1:\n",
    "    print(\"Unable to locate starting and/or ending record\\n\")\n",
    "else:\n",
    "    print(\"Optional record sections present:\")\n",
    "    if sections_seen & RDAC1_PRESENT:\n",
    "        print(\" RDAC1\")\n",
    "    if sections_seen & RDAC2_PRESENT:\n",
    "        print(\" RDAC2\")\n",
    "    if sections_seen & ATTITUDE_PRESENT:\n",
    "        print(\" Attitude\")\n",
    "    if sections_seen & GPS_PRESENT:\n",
    "        print(\" GPS\")\n",
    "\n",
    "    # Write the output to a file called \"filename.txt\"\n",
    "    with open(f\"{filename}.txt\", \"w\") as file:\n",
    "        mgl_header = parser.print_header(delimiter_type, sections_seen)\n",
    "\n",
    "        parser.bad_packets = 0\n",
    "\n",
    "        if reversals > 1:\n",
    "            print(\n",
    "        f\"\\nWARNING: the record timestamps are inconsistent, there are {reversals} groups of\\n\"\n",
    "         \"contiguous records. All records will be dumped in the order they appear in\\n\"\n",
    "        \"the file instead of from oldest to newest.\\n\\n\".format(reversals + 1)\n",
    "            )\n",
    "    \n",
    "            parser.logfile.seek(0, 0)\n",
    "        else:\n",
    "            parser.logfile.seek(oldest_pos, 0)\n",
    "\n",
    "        print(f\"Dumping packets from:{parser.logfile.tell()} please wait\")\n",
    "\n",
    "        mgl_records1 = parser.process_packets(delimiter_type, sys.maxsize, sections_seen)\n",
    "        mgl_records2 = 0\n",
    "\n",
    "        if reversals <= 1 and newest_pos < oldest_pos:\n",
    "            print(f\"More packets from:  {parser.logfile.tell()} please wait\")\n",
    "            mgl_records2 = parser.process_packets(delimiter_type, oldest_pos, sections_seen)\n",
    "\n",
    "# Creating a Pandas DataFrame using the headers and mgl_records\n",
    "mgl_df = pd.DataFrame(mgl_records1, columns=mgl_header)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(mgl_df)\n",
    "parser.logfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f796bd38-921e-4def-89c9-139b79e29a0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Time   alt  baro ASI TAS   VSI    g-s rotor   m-v  \\\n",
      "0      \"2018/03/23 08:48:22\"  1695  3123  32  32     4   99.9     0  13.1   \n",
      "1      \"2018/03/23 08:48:23\"  1697  3123  39  40   309   11.3     0  13.1   \n",
      "2      \"2018/03/23 08:48:24\"  1696  3123  44  45  -111  -35.6     0  13.1   \n",
      "3      \"2018/03/23 08:48:25\"  1696  3123  49  50    53   83.0     0  13.2   \n",
      "4      \"2018/03/23 08:48:26\"  1695  3123  53  54   -39  -99.9     0  13.2   \n",
      "...                      ...   ...   ...  ..  ..   ...    ...   ...   ...   \n",
      "46295  \"2018/06/05 14:59:30\"  1427  3157   0   0   -90    0.0     0  12.2   \n",
      "46296  \"2018/06/05 14:59:31\"  1429  3157   0   0    82    0.0     0  12.1   \n",
      "46297  \"2018/06/05 14:59:32\"  1428  3157   0   0  -101    0.0     0  12.1   \n",
      "46298  \"2018/06/05 14:59:33\"  1431  3157   0   0   170    0.0     0  12.2   \n",
      "46299  \"2018/06/05 14:59:34\"  1430  3157   0   0   -69    0.0     0  12.2   \n",
      "\n",
      "        b-v  ...    turn        lat       long   hdg  gs g-alt status sats  \\\n",
      "0      12.6  ...   -4:00  -35.30131  149.18610  1319  33  1883      3   10   \n",
      "1      12.5  ...  -14:00  -35.30141  149.18623  1308  36  1883      3   11   \n",
      "2      12.5  ...   -4:00  -35.30151  149.18639  1295  40  1883      3   11   \n",
      "3      12.4  ...   -4:00  -35.30162  149.18654  1296  44  1883      3   11   \n",
      "4      12.4  ...    1:00  -35.30174  149.18671  1296  47  1883      3   11   \n",
      "...     ...  ...     ...        ...        ...   ...  ..   ...    ...  ...   \n",
      "46295  11.6  ...    3:00  -35.30435  149.19048  3108  24  1868      3   13   \n",
      "46296  11.6  ...    1:00  -35.30429  149.19038  3115  22  1868      3   13   \n",
      "46297  11.6  ...  -30:00  -35.30426  149.19035  3114  20  1868      3   13   \n",
      "46298  11.6  ...  -63:00  -35.30418  149.19023  3036  20  1868      3   13   \n",
      "46299  11.6  ...  -70:00  -35.30415  149.19014  2964  19  1868      3   12   \n",
      "\n",
      "      hac vac  \n",
      "0       5   7  \n",
      "1       5   7  \n",
      "2       5   7  \n",
      "3       5   7  \n",
      "4       5   7  \n",
      "...    ..  ..  \n",
      "46295   5   7  \n",
      "46296   5   7  \n",
      "46297   5   7  \n",
      "46298   5   7  \n",
      "46299   5   7  \n",
      "\n",
      "[46300 rows x 62 columns]\n"
     ]
    }
   ],
   "source": [
    "import folium\n",
    "# Create a Folium map centered on Australia (latitude: -25.2744, longitude: 133.7751)\n",
    "map_center = [-35.30131, 149.18610]\n",
    "m = folium.Map(location=map_center, zoom_start=5)\n",
    "\n",
    "# Loop through the DataFrame and add points to the map\n",
    "for i, row in mgl_df.iloc[::360].iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['lat'], row['long']],\n",
    "        popup=f\"{row['Time']}\",  # Customize popup as needed\n",
    "        icon=folium.Icon(icon='info-sign', color='blue')  # Set icon and color\n",
    "    ).add_to(m)\n",
    "\n",
    "# Display the map (if you're in a Jupyter environment, this will render the map)\n",
    "m.save('australia_map.html')  # Optionally, save the map to an HTML file\n",
    "\n",
    "# To display in Jupyter Lab, simply return the map object\n",
    "m\n",
    "print(mgl_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3b4ad956-2b40-40ef-9270-36f3d31016a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
       "&lt;html&gt;\n",
       "&lt;head&gt;\n",
       "    \n",
       "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
       "    \n",
       "        &lt;script&gt;\n",
       "            L_NO_TOUCH = false;\n",
       "            L_DISABLE_3D = false;\n",
       "        &lt;/script&gt;\n",
       "    \n",
       "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
       "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://code.jquery.com/jquery-1.12.4.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
       "    \n",
       "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
       "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
       "            &lt;style&gt;\n",
       "                #map_905a22f3d7a69359333ca5f8ab20eda7 {\n",
       "                    position: relative;\n",
       "                    width: 100.0%;\n",
       "                    height: 100.0%;\n",
       "                    left: 0.0%;\n",
       "                    top: 0.0%;\n",
       "                }\n",
       "                .leaflet-container { font-size: 1rem; }\n",
       "            &lt;/style&gt;\n",
       "        \n",
       "&lt;/head&gt;\n",
       "&lt;body&gt;\n",
       "    \n",
       "    \n",
       "            &lt;div class=&quot;folium-map&quot; id=&quot;map_905a22f3d7a69359333ca5f8ab20eda7&quot; &gt;&lt;/div&gt;\n",
       "        \n",
       "&lt;/body&gt;\n",
       "&lt;script&gt;\n",
       "    \n",
       "    \n",
       "            var map_905a22f3d7a69359333ca5f8ab20eda7 = L.map(\n",
       "                &quot;map_905a22f3d7a69359333ca5f8ab20eda7&quot;,\n",
       "                {\n",
       "                    center: [-35.30131, 149.1861],\n",
       "                    crs: L.CRS.EPSG3857,\n",
       "                    zoom: 5,\n",
       "                    zoomControl: true,\n",
       "                    preferCanvas: false,\n",
       "                }\n",
       "            );\n",
       "\n",
       "            \n",
       "\n",
       "        \n",
       "    \n",
       "            var tile_layer_15b2b20e58c42f75c153573ac7195214 = L.tileLayer(\n",
       "                &quot;https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
       "                {&quot;attribution&quot;: &quot;Data by \\u0026copy; \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://openstreetmap.org\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e, under \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://www.openstreetmap.org/copyright\\&quot;\\u003eODbL\\u003c/a\\u003e.&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
       "            ).addTo(map_905a22f3d7a69359333ca5f8ab20eda7);\n",
       "        \n",
       "    \n",
       "            var marker_ce6409d56ae1e3c9a067031587a69293 = L.marker(\n",
       "                [-35.30131, 149.1861],\n",
       "                {}\n",
       "            ).addTo(map_905a22f3d7a69359333ca5f8ab20eda7);\n",
       "        \n",
       "    \n",
       "            var icon_89bad350cf030e159a8089029db1ecff = L.AwesomeMarkers.icon(\n",
       "                {&quot;extraClasses&quot;: &quot;fa-rotate-0&quot;, &quot;icon&quot;: &quot;info-sign&quot;, &quot;iconColor&quot;: &quot;white&quot;, &quot;markerColor&quot;: &quot;blue&quot;, &quot;prefix&quot;: &quot;glyphicon&quot;}\n",
       "            );\n",
       "            marker_ce6409d56ae1e3c9a067031587a69293.setIcon(icon_89bad350cf030e159a8089029db1ecff);\n",
       "        \n",
       "    \n",
       "        var popup_e6e5c805b4e702f9b9450da786aa716c = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_72ec91385deda5ae383f95584e655fa5 = $(`&lt;div id=&quot;html_72ec91385deda5ae383f95584e655fa5&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;Canberra&lt;/div&gt;`)[0];\n",
       "                popup_e6e5c805b4e702f9b9450da786aa716c.setContent(html_72ec91385deda5ae383f95584e655fa5);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_ce6409d56ae1e3c9a067031587a69293.bindPopup(popup_e6e5c805b4e702f9b9450da786aa716c)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "&lt;/script&gt;\n",
       "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x180036710>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_center = [-35.30131, 149.18610]  # Centering the map on Australia\n",
    "m = folium.Map(location=map_center, zoom_start=5)\n",
    "\n",
    "# Add your point\n",
    "folium.Marker(\n",
    "    location=[-35.30131, 149.18610],  # Canberra coordinates for example\n",
    "    popup=\"Canberra\",\n",
    "    icon=folium.Icon(icon='info-sign', color='blue')\n",
    ").add_to(m)\n",
    "\n",
    "m.save('australia_map.html')  # Save the map to view in browser\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bfaa76d-b8e0-4ce4-8929-e8d97e4df96e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 52428800 bytes\n",
      "Last byte: b'\\x00'\n"
     ]
    }
   ],
   "source": [
    "# Open the file in binary mode (or text mode if working with text files)\n",
    "with open('2018-6-27-Enigma.rec', 'rb') as f:\n",
    "    # Move the file pointer to the end of the file\n",
    "    f.seek(0, 2)  # 0 bytes relative to the end of the file\n",
    "    file_size = f.tell()  # Get the current position, which is the file size in bytes\n",
    "    print(f\"File size: {file_size} bytes\")\n",
    "    \n",
    "    # Read the last byte\n",
    "    f.seek(-1, 2)  # Move to 1 byte before the end of the file\n",
    "    last_byte = f.read(1)  # Read the last byte\n",
    "    print(f\"Last byte: {last_byte}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82215cc0-e48f-404e-bc9a-c4d05bd5ddd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AA 55 8B 1E D6 7B 47 22 9F 06 00 00 33 0C 20 00 20 00 04 00 E7 03 00 80 83 7E 00 00 B4 00 0D 06 42 01 01 3F 9A 09 BC 08 C0 08 1A 00 3B 00 1E 12 AB 03 1E 00 1E 00 30 04 3B 00 18 00 00 00 03 19 17 02 AD 01 16 02 D2 01 19 02 0E 02 84 00 80 00 76 00 74 00 6F 00 73 00 AC 05 E1 01 E3 01 11 06 62 09 00 03 0E 0A 00 6F 00 F4 FF 18 05 82 00 0A 00 FC FF 04 18 8B 34 0D C2 A4 2F 15 43 27 05 00 00 21 00 00 00 5B 07 00 00 03 0A 05 07\n",
      "AA 55 8B 1E D7 7B 47 22 A1 06 00 00 33 0C 27 00 28 00 35 01 71 00 00 80 83 7D 00 00 B4 00 0D 06 86 01 01 3F 96 0A C0 08 C0 08 1A 00 29 00 1E 12 AD 03 1E 00 1E 00 30 04 3A 00 25 00 00 00 16 19 1C 02 B6 01 1E 02 DD 01 28 02 17 02 86 00 82 00 76 00 74 00 6F 00 73 00 A7 05 AE 03 E4 01 87 08 63 09 00 03 0E 0A 00 76 00 F9 FF 36 05 85 00 0A 00 F2 FF 04 18 A4 34 0D C2 AD 2F 15 43 1C 05 00 00 24 00 00 00 5B 07 00 00 03 0B 05 07\n",
      "AA 55 8B 1E D8 7B 47 22 A0 06 00 00 33 0C 2C 00 2D 00 91 FF 9C FE 00 80 83 7D 00 00 63 00 0D 06 B8 01 01 3F EF 0A BD 08 C0 08 1A 00 38 00 73 10 A3 03 1E 00 1E 00 30 04 3A 00 1D 00 00 00 05 19 1E 02 B4 01 20 02 E6 01 28 02 17 02 86 00 82 00 78 00 76 00 72 00 75 00 AF 05 19 02 E4 01 FB 06 63 09 00 03 0E 0A 00 7E 00 FC FF 39 05 85 00 0A 00 FC FF 04 18 BE 34 0D C2 B7 2F 15 43 0F 05 00 00 28 00 00 00 5B 07 00 00 03 0B 05 07\n",
      "AA 55 8B 1E D9 7B 47 22 A0 06 00 00 33 0C 31 00 32 00 35 00 3E 03 00 80 84 7C 00 00 4C 00 0D 06 EA 01 01 3F EF 0A BD 08 C0 08 1A 00 38 00 73 10 9F 03 1E 00 1E 00 30 04 3A 00 1D 00 00 00 05 19 1E 02 B4 01 20 02 E6 01 35 02 19 02 87 00 82 00 79 00 78 00 74 00 76 00 AF 05 19 02 E4 01 FB 06 63 09 00 03 0E 14 00 87 00 FA FF 35 05 85 00 0A 00 FC FF 04 18 DC 34 0D C2 C1 2F 15 43 10 05 00 00 2C 00 00 00 5B 07 00 00 03 0B 05 07\n",
      "AA 55 8B 1E DA 7B 47 22 9F 06 00 00 33 0C 35 00 36 00 D9 FF 19 FC 00 80 84 7C 00 00 3D 00 0D 06 19 02 01 3F EF 0A BC 08 C0 08 1A 00 39 00 73 10 9C 03 1E 00 1E 00 30 04 3B 00 1D 00 00 00 04 19 1E 02 B1 01 22 02 EE 01 35 02 19 02 87 00 82 00 79 00 78 00 74 00 76 00 A2 05 07 02 E3 01 0C 07 64 09 00 03 0E 14 00 8B 00 F8 FF 29 05 83 00 0A 00 01 00 04 18 FB 34 0D C2 CC 2F 15 43 10 05 00 00 2F 00 00 00 5B 07 00 00 03 0B 05 07\n",
      "AA 55 8B 1E DB 7B 47 22 9B 06 00 00 33 0C 39 00 3A 00 ED FE 47 FF 00 80 83 7D 00 00 34 00 0D 06 3B 02 01 3F EF 0A BC 08 C0 08 1A 00 39 00 73 10 A2 03 1E 00 1E 00 30 04 3B 00 1D 00 00 00 04 19 1E 02 B1 01 22 02 EE 01 45 02 1A 02 87 00 81 00 79 00 78 00 74 00 76 00 A2 05 07 02 E3 01 0C 07 64 09 00 03 0E 0A 00 8B 00 F7 FF 24 05 84 00 0A 00 00 00 04 18 1C 35 0D C2 D9 2F 15 43 11 05 00 00 33 00 00 00 5C 07 00 00 03 0B 05 07\n",
      "AA 55 8B 1E DC 7B 47 22 9A 06 00 00 33 0C 3D 00 3E 00 20 00 E7 03 00 80 83 7D 00 00 2B 00 0D 06 67 02 01 3F EF 0A BC 08 C0 08 1B 00 3A 00 A6 10 B1 03 1E 00 1E 00 2F 04 3A 00 30 00 00 00 03 19 1F 02 B2 01 28 02 F8 01 45 02 1A 02 87 00 81 00 79 00 78 00 74 00 76 00 96 05 F7 01 E4 01 8D 0A 64 09 00 03 0E 0A 00 83 00 F6 FF 21 05 83 00 0A 00 FC FF 04 18 40 35 0D C2 E6 2F 15 43 14 05 00 00 36 00 00 00 5C 07 00 00 03 0B 05 07\n",
      "AA 55 8B 1E DD 7B 47 22 9C 06 00 00 33 0C 42 00 43 00 C6 00 29 01 00 80 83 7C 00 00 29 00 0D 06 95 02 01 3F EF\n",
      "34 1\n",
      "RADC 1\n",
      "ATT\n",
      "GPS\n",
      "141 2 26\n",
      "26 141\n"
     ]
    }
   ],
   "source": [
    "with open('2018-6-27-Enigma.rec', 'rb') as f:  # Open the file in binary mode\n",
    "    packet = f.read(1024)  # Read the first 8 bytes\n",
    "\n",
    "# Convert each byte to its hexadecimal representation and join them with a space\n",
    "hex_output = ' '.join(f'{byte:02X}' for byte in packet)\n",
    "# Set the block size (number of bytes per line)\n",
    "block_size = 141\n",
    "\n",
    "# Split the hex output into blocks of 'block_size' and print each block on a new line\n",
    "hex_blocks = hex_output.split()  # Convert hex string to a list of hex values\n",
    "for i in range(0, len(hex_blocks), block_size):\n",
    "    # Print each block (8 hex values per line)\n",
    "    print(' '.join(hex_blocks[i:i + block_size]))\n",
    "\n",
    "payload = packet\n",
    "# packet[0] is the total payload length + fudge factor\n",
    "# packet[1] is an offset\n",
    "sections_seen = 0\n",
    "rdac1_len = 0\n",
    "rdac2_len = 0\n",
    "attitude_len = 0\n",
    "gps_len = 0\n",
    "\n",
    "revision = 2\n",
    "p_offset = section_sizes[0][revision]+ HEADER_LEN + fudge_factors[revision]\n",
    "p_idx = p_offset\n",
    "payload_len = 141\n",
    "\n",
    "# Parse sections\n",
    "print(p_idx, payload[p_idx])\n",
    "if p_idx < payload_len and payload[p_idx] == rdac1_data_tag:\n",
    "    sections_seen |= RDAC1_PRESENT\n",
    "    print('RADC 1')\n",
    "    tag = payload[p_idx]\n",
    "    p_idx += 1\n",
    "    rdac1_len = payload[p_idx]\n",
    "    p_idx += 1\n",
    "    p_idx += rdac1_len\n",
    "\n",
    "if p_idx < payload_len and payload[p_idx] == rdac2_data_tag:\n",
    "    sections_seen |= RDAC2_PRESENT\n",
    "    print('RADC 2')\n",
    "    tag = payload[p_idx]\n",
    "    p_idx += 1\n",
    "    rdac2_len = payload[p_idx]\n",
    "    p_idx += 1\n",
    "    p_idx += rdac2_len\n",
    "\n",
    "if p_idx < payload_len and payload[p_idx] == attitude_data_tag:\n",
    "    sections_seen |= ATTITUDE_PRESENT\n",
    "    print('ATT')\n",
    "    tag = payload[p_idx]\n",
    "    p_idx += 1\n",
    "    attitude_len = payload[p_idx]\n",
    "    p_idx += 1\n",
    "    p_idx += attitude_len\n",
    "\n",
    "if p_idx < payload_len and payload[p_idx] == gps_data_tag:\n",
    "    sections_seen |= GPS_PRESENT\n",
    "    print('GPS')\n",
    "    tag = payload[p_idx]\n",
    "    p_idx += 1\n",
    "    gps_len = payload[p_idx]\n",
    "    p_idx += 1\n",
    "    p_idx += gps_len\n",
    "\n",
    "if (p_idx == payload_len and\n",
    "    ((sections_seen & RDAC1_PRESENT == 0) or rdac1_len == section_sizes[rdac1_section][revision]) and\n",
    "    ((sections_seen & RDAC2_PRESENT == 0) or rdac2_len == section_sizes[rdac2_section][revision]) and\n",
    "    ((sections_seen & ATTITUDE_PRESENT == 0) or attitude_len == section_sizes[attitude_section][revision]) and\n",
    "    ((sections_seen & GPS_PRESENT == 0) or gps_len == section_sizes[gps_section][revision])\n",
    "   ):\n",
    "    print( payload_len, revision, sections_seen)\n",
    "\n",
    "print( sections_seen, payload_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3fa4c3-02d1-4233-9d6c-2e18b4037d7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### timestamp = 569203200 #512457686\n",
    "#           0    1    2    3    4   5   6     7\n",
    "packet = [0xaa,0x55,0x8b,0x1e,0xd6,0x7b,0x47,0x22,0x9f]\n",
    "timestamp = (\n",
    "                packet[7] << 24 |\n",
    "                packet[6] << 16 |\n",
    "                packet[5] << 8 |\n",
    "                packet[4])\n",
    "base_time = datetime.datetime(2000, 1, 1)\n",
    "adj_time = base_time + datetime.timedelta(seconds=timestamp)\n",
    "time_str = adj_time.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "output = [f'\"{time_str}\"']\n",
    "print(f'Stamp:{timestamp}   Base:{base_time}  Output:{output}')\n",
    "print(packet[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb612438-aab9-478b-84e1-8ee795df6447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
